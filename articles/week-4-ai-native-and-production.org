#+TITLE: Week 4+ Articles - AI-Native Patterns & Production
#+AUTHOR: MCP Mesh Team
#+DATE: 2024-12
#+OPTIONS: toc:2 num:t

* Overview

Week 4+ covers advanced AI patterns and production deployment:

*AI-Native Patterns (Week 4)*
- Article 9: Agents calling agents
- Article 10: LLM failover as topology change
- Article 11: Structured LLM output with Pydantic
- Article 12: Context propagation with Jinja2

*Production (Week 5)*
- Article 13: Observability with Grafana and Tempo
- Article 14: Laptop to GKE deployment
- Article 15: Schema isolation for autonomous agents

* Article 9: Agents Calling Agents
:PROPERTIES:
:EXPORT_TITLE: Agents Calling Agents â€” Emergent Collaboration
:EXPORT_TAGS: ai-agents, multi-agent, mcp, collaboration
:END:

** Meta

- *Goal*: Show how agents discover and collaborate without central coordination
- *Tone*: Visionary but grounded
- *Length*: ~1300 words

** Article Draft

*** Introduction

Traditional multi-agent systems have a coordinator:

#+begin_src
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Coordinator  â”‚
                    â”‚  (knows all)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼             â–¼             â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Agent  â”‚    â”‚ Agent  â”‚    â”‚ Agent  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#+end_src

The coordinator decides who does what. Agents are workers, not collaborators.

MCP Mesh enables a different pattern: agents discover each other and form collaborations dynamically.

*** The Hiring Platform Example

In our hiring platform, the interview agent needs to:
1. Get job requirements
2. Get candidate profile
3. Generate questions (LLM)
4. Score responses (LLM)
5. Update application status

Here's how it declares these needs:

#+begin_src python
@mesh.tool(
    capability="conduct_interview",
    dependencies=[
        {"capability": "job_details"},
        {"capability": "candidate_profile"},
        {"capability": "llm", "tags": ["+claude"]},
        {"capability": "update_application"}
    ]
)
async def conduct_interview(
    job_id: str,
    candidate_id: str,
    job_details: McpMeshAgent = None,
    candidate_profile: McpMeshAgent = None,
    llm: McpMeshAgent = None,
    update_application: McpMeshAgent = None
) -> InterviewResult:

    job = await job_details(job_id=job_id)
    candidate = await candidate_profile(candidate_id=candidate_id)

    # Generate questions using LLM
    questions = await llm(
        f"Generate interview questions for {job.title} "
        f"based on candidate background: {candidate.summary}"
    )

    # ... conduct interview ...

    await update_application(
        candidate_id=candidate_id,
        status="interviewed",
        score=final_score
    )

    return InterviewResult(...)
#+end_src

The interview agent doesn't know:
- Where job details come from
- How candidate profiles are stored
- Which LLM will answer
- How application status is persisted

It knows *what* it needs. The mesh provides *who* can help.

*** Emergent Collaboration

Deploy a new agent that provides =candidate_scoring=:

#+begin_src python
@mesh.tool(
    capability="candidate_scoring",
    tags=["ml", "production"],
    dependencies=[
        {"capability": "llm", "tags": ["+openai"]}
    ]
)
async def score_candidate(
    resume: str,
    job_requirements: list,
    llm: McpMeshAgent = None
) -> ScoreResult:
    ...
#+end_src

Update interview agent to use it:

#+begin_src python
dependencies=[
    ...,
    {"capability": "candidate_scoring"}
]
#+end_src

Restart interview agent. It discovers the scoring agent automatically. The collaboration emerged from declared needs, not central configuration.

*** Chain of Agents

Agents can call agents that call agents:

#+begin_src
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application â”‚â”€â”€â”€â”€â–ºâ”‚  Interview  â”‚â”€â”€â”€â”€â–ºâ”‚   Scoring   â”‚
â”‚   Agent     â”‚     â”‚    Agent    â”‚     â”‚    Agent    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚                   â”‚
                           â–¼                   â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Job Details â”‚     â”‚     LLM     â”‚
                    â”‚   Agent     â”‚     â”‚  Provider   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#+end_src

Each agent only knows its immediate dependencies. The full collaboration graph emerges from individual declarations.

*** Fan-Out Pattern

An agent can call multiple providers of the same capability:

#+begin_src python
@mesh.tool(
    capability="aggregate_scores",
    dependencies=[
        {"capability": "candidate_scoring", "filter_mode": "all"}
    ]
)
async def aggregate_scores(
    candidate_id: str,
    scoring_agents: list[McpMeshAgent] = None
) -> AggregateScore:

    # Call all scoring agents in parallel
    scores = await asyncio.gather(*[
        agent(candidate_id=candidate_id)
        for agent in scoring_agents
    ])

    return AggregateScore(
        average=sum(s.score for s in scores) / len(scores),
        scores=scores
    )
#+end_src

The =filter_mode: "all"= returns every matching provider, enabling ensemble patterns.

*** Self-Healing Collaboration

When an agent in the chain fails:

#+begin_src
Timeline:
0s   - Interview agent calls scoring agent
5s   - Scoring agent processes
10s  - Scoring agent crashes
15s  - Scoring agent missed heartbeats
20s  - Mesh marks scoring agent unhealthy
25s  - Interview agent's next call:
       - If backup scoring agent exists â†’ routes there
       - If no backup â†’ dependency returns None
30s  - Interview agent handles gracefully:
       if scoring_agent:
           score = await scoring_agent(...)
       else:
           score = default_score()
#+end_src

The collaboration adapts. No central coordinator needed.

*** Versus Orchestrated Workflows

| Aspect | Orchestrator | MCP Mesh |
|--------|--------------|----------|
| Workflow definition | Central (YAML/code) | Distributed (dependencies) |
| Adding steps | Edit orchestrator | Deploy new agent |
| Removing steps | Edit orchestrator | Stop agent |
| Failure handling | Orchestrator logic | Mesh + agent logic |
| Visibility | Orchestrator dashboard | Distributed traces |
| Coupling | Tight to orchestrator | Loose (capabilities) |

Use orchestrators when you need strict workflow ordering and central control.

Use MCP Mesh when agents should self-organize based on available capabilities.

*** What's Next

Collaboration depends on healthy agents. When an LLM provider fails, the mesh handles failover automatically.

Next article: LLM failover as a topology change.

ðŸ‘‰ [Next: LLM failover as topology change]


* Article 10: LLM Failover as Topology Change
:PROPERTIES:
:EXPORT_TITLE: LLM Failover as Topology Change
:EXPORT_TAGS: llm, failover, reliability, mcp
:END:

** Meta

- *Goal*: Show that LLM failover is just normal mesh behavior
- *Tone*: Technical, reassuring
- *Length*: ~1100 words

** Article Draft

*** Introduction

Most LLM integrations handle failover like this:

#+begin_src python
def call_llm(prompt):
    try:
        return call_claude(prompt)
    except ClaudeAPIError:
        try:
            return call_openai(prompt)
        except OpenAIAPIError:
            try:
                return call_local_llm(prompt)
            except:
                raise AllProvidersFailedError()
#+end_src

Hardcoded fallback chains. Manual exception handling. Changes require code updates.

MCP Mesh handles LLM failover as a topology change â€” the same mechanism that handles any agent failure.

*** How It Works

*Setup: Two LLM providers*

#+begin_src python
# Provider 1: Claude
@mesh.llm_provider(
    model="anthropic/claude-sonnet-4-20250514",
    capability="llm",
    tags=["claude", "production"]
)

# Provider 2: OpenAI
@mesh.llm_provider(
    model="openai/gpt-4o",
    capability="llm",
    tags=["openai", "production"]
)
#+end_src

*Consumer: Prefers Claude*

#+begin_src python
@mesh.tool(
    capability="analyze_text",
    dependencies=[
        {"capability": "llm", "tags": ["+production", "claude"]}
    ]
)
async def analyze_text(
    text: str,
    llm: McpMeshAgent = None
) -> Analysis:
    return await llm(f"Analyze: {text}")
#+end_src

The =claude= tag is a soft preference (no =+=). The consumer will accept any =+production= LLM.

*** Failure Scenario

#+begin_src
Timeline:

00:00  Both providers healthy
       analyze_text â†’ routes to claude-provider

00:30  Claude API returns 503 errors
       claude-provider starts failing health checks

00:35  claude-provider misses heartbeat #1
00:40  claude-provider misses heartbeat #2
00:45  claude-provider misses heartbeat #3
00:50  claude-provider misses heartbeat #4

00:50  Registry marks claude-provider UNHEALTHY
       Topology version: 42 â†’ 43

00:55  analyze_text agent sends heartbeat
       Receives topology v43
       Dependency "llm" re-resolved â†’ openai-provider

01:00  Next analyze_text call â†’ routes to openai-provider
       No code change. No config change. Automatic.
#+end_src

*** What the Consumer Sees

#+begin_src python
# Before failover
result = await llm("Analyze this text")
# Routed to Claude, got Claude response

# After failover (same code)
result = await llm("Analyze this text")
# Routed to OpenAI, got OpenAI response
#+end_src

The consumer code doesn't change. The mesh handled the routing.

*** Recovery

When Claude recovers:

#+begin_src
Timeline:

05:00  Claude API healthy again
       claude-provider sends heartbeat

05:00  Registry marks claude-provider HEALTHY
       Topology version: 43 â†’ 44

05:05  analyze_text agent heartbeat
       Receives topology v44
       Dependency "llm" re-resolved â†’ claude-provider (preferred)

05:10  Next call routes to Claude again
#+end_src

Automatic recovery. The consumer never knew there was an outage.

*** Configuring Failover Behavior

*Hard requirement (no failover)*

#+begin_src python
dependencies=[
    {"capability": "llm", "tags": ["+claude"]}  # Plus means required
]
#+end_src

If Claude is unhealthy, =llm= dependency becomes =None=. Handle explicitly:

#+begin_src python
if llm is None:
    raise LLMUnavailableError("Claude is required but unavailable")
#+end_src

*Ordered preference*

Tag order matters:

#+begin_src python
dependencies=[
    {"capability": "llm", "tags": ["+production", "claude", "openai", "local"]}
]
#+end_src

Prefers Claude, then OpenAI, then local. All must be production.

*Any provider*

#+begin_src python
dependencies=[
    {"capability": "llm", "tags": ["+production"]}
]
#+end_src

Any production LLM works. Mesh picks the healthiest.

*** Why This Is Better

| Traditional Failover | Mesh Failover |
|---------------------|---------------|
| Hardcoded in code | Declared in dependencies |
| Exception-based | Health-based |
| Immediate (per-request) | Proactive (heartbeat-based) |
| Manual recovery | Automatic recovery |
| Provider-specific code | Uniform interface |

The mesh knows about failures *before* you make a request that would fail.

*** What's Next

LLM calls return text. But AI applications need structure. Next article: Enforcing typed responses with Pydantic.

ðŸ‘‰ [Next: Structured LLM output with Pydantic]


* Article 11: Structured LLM Output with Pydantic
:PROPERTIES:
:EXPORT_TITLE: Structured LLM Output with Pydantic
:EXPORT_TAGS: llm, pydantic, python, type-safety
:END:

** Meta

- *Goal*: Show how return type hints enforce LLM output structure
- *Tone*: Practical, code-focused
- *Length*: ~1200 words

** Article Draft

*** Introduction

LLMs return strings. Applications need structure.

The traditional approach:

#+begin_src python
response = await llm("Analyze this resume")
# response = "The candidate has 5 years of experience..."

# Parse manually
try:
    data = json.loads(response)
    name = data.get("name", "Unknown")
    years = data.get("experience_years", 0)
except json.JSONDecodeError:
    # LLM didn't return JSON, try regex
    match = re.search(r"(\d+) years", response)
    years = int(match.group(1)) if match else 0
#+end_src

Fragile. Error-prone. Changes with every prompt tweak.

MCP Mesh uses return type hints to enforce structure.

*** The Pattern

#+begin_src python
from pydantic import BaseModel, Field

class ResumeAnalysis(BaseModel):
    name: str = Field(description="Candidate's full name")
    experience_years: int = Field(description="Total years of experience")
    skills: list[str] = Field(description="Technical skills")
    summary: str = Field(description="Brief summary")

@mesh.tool(capability="analyze_resume")
@mesh.llm(provider={"capability": "llm"})
async def analyze_resume(
    resume_text: str,
    llm: MeshLlmAgent = None
) -> ResumeAnalysis:  # Return type enforced

    result = await llm(f"Analyze this resume:\n\n{resume_text}")
    return result  # Automatically parsed and validated
#+end_src

The mesh:
1. Sees =ResumeAnalysis= return type
2. Generates JSON schema from Pydantic model
3. Instructs LLM to return JSON matching schema
4. Parses response into =ResumeAnalysis= instance
5. Raises validation error if response doesn't match

*** How It Works

*Step 1: Schema Generation*

Pydantic model becomes JSON schema:

#+begin_src json
{
  "type": "object",
  "properties": {
    "name": {"type": "string", "description": "Candidate's full name"},
    "experience_years": {"type": "integer", "description": "Total years of experience"},
    "skills": {"type": "array", "items": {"type": "string"}},
    "summary": {"type": "string", "description": "Brief summary"}
  },
  "required": ["name", "experience_years", "skills", "summary"]
}
#+end_src

*Step 2: LLM Instruction*

Mesh tells the LLM provider to request structured output. For OpenAI:

#+begin_src python
response = client.chat.completions.create(
    model="gpt-4o",
    response_format={"type": "json_schema", "json_schema": schema},
    messages=[...]
)
#+end_src

For Claude:

#+begin_src python
response = client.messages.create(
    model="claude-sonnet-4-20250514",
    messages=[...],
    # Schema provided in system prompt
)
#+end_src

*Step 3: Validation*

Response is validated against the Pydantic model:

#+begin_src python
try:
    result = ResumeAnalysis.model_validate_json(response)
except ValidationError as e:
    # Clear error: "experience_years must be integer, got string"
    raise StructuredOutputError(e)
#+end_src

*** Nested Models

Complex structures work too:

#+begin_src python
class Skill(BaseModel):
    name: str
    level: Literal["beginner", "intermediate", "expert"]
    years: int

class Education(BaseModel):
    degree: str
    institution: str
    year: int

class DetailedAnalysis(BaseModel):
    name: str
    skills: list[Skill]
    education: list[Education]
    recommendations: list[str]

@mesh.llm(provider={"capability": "llm"})
async def detailed_analysis(
    resume: str,
    llm: MeshLlmAgent = None
) -> DetailedAnalysis:
    return await llm(f"Provide detailed analysis:\n\n{resume}")
#+end_src

The LLM returns nested JSON. Pydantic validates the entire structure.

*** Optional Fields and Defaults

#+begin_src python
class FlexibleAnalysis(BaseModel):
    name: str  # Required
    email: Optional[str] = None  # Optional
    experience_years: int = 0  # Default if missing
    skills: list[str] = Field(default_factory=list)
#+end_src

Mesh handles missing fields gracefully.

*** Enums and Literals

Constrain values:

#+begin_src python
from enum import Enum

class ExperienceLevel(str, Enum):
    JUNIOR = "junior"
    MID = "mid"
    SENIOR = "senior"
    STAFF = "staff"

class CandidateLevel(BaseModel):
    level: ExperienceLevel
    confidence: float = Field(ge=0, le=1)  # Between 0 and 1
#+end_src

LLM must return valid enum values. Validation catches mistakes.

*** Error Handling

When LLM returns invalid structure:

#+begin_src python
try:
    result = await analyze_resume(resume_text)
except StructuredOutputError as e:
    # e.validation_errors: List of what went wrong
    # e.raw_response: What the LLM actually returned
    logger.error(f"LLM output validation failed: {e.validation_errors}")
    # Retry with different prompt, or handle gracefully
#+end_src

Clear errors instead of silent failures.

*** Performance Note

Structured output adds ~10-20% latency compared to free-form text:
- Schema included in prompt (more tokens)
- JSON parsing on response
- Validation step

Worth it for reliability. Skip it for simple text generation.

*** What's Next

Structured output handles the response. Context injection handles the input â€” dynamic prompts that adapt to runtime data.

ðŸ‘‰ [Next: Context propagation with Jinja2]


* Article 12: Context Propagation with Jinja2
:PROPERTIES:
:EXPORT_TITLE: Context Propagation with Jinja2 Templates
:EXPORT_TAGS: llm, prompts, jinja2, templates
:END:

** Meta

- *Goal*: Show how to build dynamic prompts with context injection
- *Tone*: Practical, template-focused
- *Length*: ~1200 words

** Article Draft

*** Introduction

Hardcoded prompts don't scale:

#+begin_src python
prompt = f"""
You are interviewing for: {job_title}
Requirements: {", ".join(requirements)}
Candidate: {candidate_name}
Resume: {resume_text}
Questions asked: {len(questions)}
"""
#+end_src

String formatting works until:
- Prompts get long (hundreds of lines)
- Logic is needed (conditionals, loops)
- Non-developers need to edit prompts
- You want to version prompts separately

MCP Mesh uses Jinja2 templates for prompt engineering.

*** Basic Template

#+begin_src python
@mesh.llm(
    provider={"capability": "llm"},
    system_prompt="file://prompts/interviewer.jinja2",
    context_param="ctx"
)
async def generate_question(
    ctx: InterviewContext,
    llm: MeshLlmAgent = None
) -> InterviewQuestion:
    return await llm("Generate the next interview question")
#+end_src

Template file (=prompts/interviewer.jinja2=):

#+begin_src jinja2
You are a technical interviewer for {{ ctx.company_name }}.

## Position
**Role:** {{ ctx.job_title }}
**Level:** {{ ctx.level }}

## Requirements
{% for req in ctx.requirements %}
- {{ req }}
{% endfor %}

## Candidate Background
{{ ctx.resume_summary }}

## Interview Progress
- Questions asked: {{ ctx.questions_asked }}
- Time remaining: {{ ctx.time_remaining_minutes }} minutes
{% if ctx.questions_asked > 0 %}

## Previous Q&A
{% for qa in ctx.history %}
**Q{{ loop.index }}:** {{ qa.question }}
**A:** {{ qa.answer }}
**Assessment:** {{ qa.assessment }}

{% endfor %}
{% endif %}

## Instructions
Generate a relevant technical question that:
1. Hasn't been asked yet
2. Matches the candidate's experience level
3. Relates to the job requirements
#+end_src

*** Context Objects

Define context with Pydantic:

#+begin_src python
class QA(BaseModel):
    question: str
    answer: str
    assessment: str

class InterviewContext(BaseModel):
    company_name: str
    job_title: str
    level: str
    requirements: list[str]
    resume_summary: str
    questions_asked: int
    time_remaining_minutes: int
    history: list[QA] = []
#+end_src

Pass context to the function:

#+begin_src python
ctx = InterviewContext(
    company_name="TechCorp",
    job_title="Senior Backend Engineer",
    level="senior",
    requirements=["Python", "Distributed Systems", "PostgreSQL"],
    resume_summary="10 years experience, strong in Python and Go...",
    questions_asked=3,
    time_remaining_minutes=25,
    history=[
        QA(
            question="Describe your experience with distributed systems",
            answer="I've built several microservice architectures...",
            assessment="Strong understanding of fundamentals"
        )
    ]
)

question = await generate_question(ctx=ctx)
#+end_src

*** Template Features

*Conditionals*

#+begin_src jinja2
{% if ctx.is_first_question %}
Start with an icebreaker question to make the candidate comfortable.
{% else %}
Build on the previous answers.
{% endif %}
#+end_src

*Loops*

#+begin_src jinja2
Required skills:
{% for skill in ctx.required_skills %}
- {{ skill.name }} ({{ skill.importance }})
{% endfor %}
#+end_src

*Filters*

#+begin_src jinja2
Summary: {{ ctx.long_text | truncate(500) }}
Skills: {{ ctx.skills | join(", ") }}
Name: {{ ctx.name | upper }}
#+end_src

*Includes*

#+begin_src jinja2
{% include 'prompts/common/safety_guidelines.jinja2' %}

## Role-Specific Instructions
...
#+end_src

*** Inline vs File Templates

*Inline* (simple prompts):

#+begin_src python
@mesh.llm(
    system_prompt="You are a {{ ctx.role }} assistant. Be {{ ctx.tone }}."
)
#+end_src

*File* (complex prompts):

#+begin_src python
@mesh.llm(
    system_prompt="file://prompts/complex_system.jinja2"
)
#+end_src

File templates are:
- Versionable in git
- Editable by non-developers
- Testable independently

*** Multi-Context

Pass multiple context objects:

#+begin_src python
@mesh.llm(
    system_prompt="file://prompts/analysis.jinja2",
    context_param="ctx"  # Primary context
)
async def analyze(
    ctx: AnalysisContext,
    user: UserContext,  # Additional context
    llm: MeshLlmAgent = None
):
    # Template can access both via ctx and user
    ...
#+end_src

Template:

#+begin_src jinja2
Analyzing for user: {{ user.name }} ({{ user.role }})

Analysis request:
{{ ctx.request }}
#+end_src

*** Template Debugging

Enable debug mode to see rendered prompts:

#+begin_src python
@mesh.llm(
    system_prompt="file://prompts/interviewer.jinja2",
    debug=True  # Logs rendered prompt
)
#+end_src

Or render manually:

#+begin_src python
from mcp_mesh.templates import render_template

rendered = render_template(
    "prompts/interviewer.jinja2",
    ctx=interview_context
)
print(rendered)  # See exactly what LLM receives
#+end_src

*** Best Practices

1. *Keep prompts in files* for anything over 5 lines
2. *Use typed context objects* (Pydantic) for validation
3. *Include examples* in templates when output format matters
4. *Version prompts* alongside code
5. *Test templates* with various context values

*** What's Next

AI patterns covered. Now for production: observability, deployment, and data isolation.

ðŸ‘‰ [Next: Observability with Grafana and Tempo]


* Article 13: Observability Built-In
:PROPERTIES:
:EXPORT_TITLE: Observability Built-In â€” Tracing Across the Mesh
:EXPORT_TAGS: observability, grafana, tempo, tracing
:END:

** Meta

- *Goal*: Show zero-config observability for distributed agents
- *Tone*: Practical, ops-focused
- *Length*: ~1200 words

** Article Draft

*** Introduction

Distributed systems are hard to debug. When a request touches 6 agents, where do you look when something fails?

MCP Mesh includes observability from day one:
- Distributed tracing (Tempo)
- Visualization (Grafana)
- Zero configuration

*** Enabling Tracing

In docker-compose or Kubernetes:

#+begin_src yaml
environment:
  MCP_MESH_DISTRIBUTED_TRACING_ENABLED: "true"
  OTEL_EXPORTER_OTLP_ENDPOINT: "http://tempo:4317"
#+end_src

That's it. Every agent automatically:
- Creates spans for tool calls
- Propagates trace context across agent calls
- Exports to Tempo via OTLP

*** What Gets Traced

Every MCP tool call becomes a span:

#+begin_src
Trace: interview-flow-abc123
â”œâ”€â”€ conduct_interview (interview-agent) [2.3s]
â”‚   â”œâ”€â”€ job_details (job-agent) [45ms]
â”‚   â”œâ”€â”€ candidate_profile (user-agent) [62ms]
â”‚   â”œâ”€â”€ generate_question (interview-agent) [890ms]
â”‚   â”‚   â””â”€â”€ llm_call (claude-provider) [850ms]
â”‚   â”œâ”€â”€ score_response (scoring-agent) [340ms]
â”‚   â”‚   â””â”€â”€ llm_call (openai-provider) [310ms]
â”‚   â””â”€â”€ update_application (application-agent) [55ms]
#+end_src

Each span includes:
- Agent name and capability
- Duration
- Input parameters (configurable)
- Output summary
- Error details (if failed)

*** Grafana Dashboard

Access Grafana at =http://localhost:3000= (default credentials: admin/admin).

*Pre-built dashboards:*

1. *Mesh Overview*
   - Active agents
   - Request rate per agent
   - Error rate
   - p50/p95/p99 latencies

2. *Trace Explorer*
   - Search by trace ID
   - Filter by agent
   - Filter by capability
   - Filter by error

3. *Agent Health*
   - Heartbeat status
   - Dependency resolution success
   - LLM provider availability

*** Finding Slow Requests

In Grafana Tempo:

#+begin_src
Query: { duration > 2s && resource.service.name = "interview-agent" }
#+end_src

Results show all interview-agent calls taking over 2 seconds. Click to see the trace breakdown.

*** Finding Errors

#+begin_src
Query: { status = error }
#+end_src

Shows all failed spans. Trace view shows:
- Which agent failed
- What it was trying to do
- The error message
- What called it

*** Correlating Logs

Agents automatically include trace ID in logs:

#+begin_src
[INFO] [trace_id=abc123] conduct_interview started
[INFO] [trace_id=abc123] Calling job_details
[INFO] [trace_id=abc123] job_details returned in 45ms
[ERROR] [trace_id=abc123] LLM call failed: rate limited
#+end_src

Search logs by trace ID to see the full story.

*** Custom Spans

Add spans for internal operations:

#+begin_src python
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

@mesh.tool(capability="complex_analysis")
async def complex_analysis(data: dict) -> Result:

    with tracer.start_as_current_span("preprocess_data"):
        processed = preprocess(data)

    with tracer.start_as_current_span("run_model"):
        prediction = model.predict(processed)

    with tracer.start_as_current_span("postprocess"):
        result = postprocess(prediction)

    return result
#+end_src

These appear as child spans in traces.

*** Production Configuration

For production, use a proper Tempo backend:

#+begin_src yaml
# Tempo with S3 storage
tempo:
  image: grafana/tempo:latest
  environment:
    - TEMPO_STORAGE_TRACE_BACKEND=s3
    - TEMPO_STORAGE_TRACE_S3_BUCKET=traces
    - TEMPO_STORAGE_TRACE_S3_ENDPOINT=s3.amazonaws.com
#+end_src

Retention, sampling, and scaling are Tempo configuration â€” MCP Mesh just exports traces.

*** What's Next

You can see what's happening. Now deploy it anywhere.

ðŸ‘‰ [Next: Laptop to GKE â€” same code, different scale]


* Article 14: Laptop to GKE
:PROPERTIES:
:EXPORT_TITLE: From Laptop to GKE â€” Same Code, Different Scale
:EXPORT_TAGS: deployment, docker, kubernetes, gke
:END:

** Meta

- *Goal*: Show that development and production use identical code
- *Tone*: DevOps-practical
- *Length*: ~1300 words

** Article Draft

*** Introduction

"Works on my machine" is the oldest problem in software.

MCP Mesh agents are the same code in development and production. Only the infrastructure changes.

*** Development: meshctl start

#+begin_src bash
# Start registry (embedded)
meshctl start

# Start agents
meshctl start agents/job_agent.py
meshctl start agents/interview_agent.py
meshctl start agents/scoring_agent.py
#+end_src

Registry runs embedded. Agents discover each other via localhost. SQLite for state.

*** Docker Compose: Local Multi-Container

#+begin_src bash
meshctl scaffold --compose --observability
docker-compose up
#+end_src

Generated =docker-compose.yml=:

#+begin_src yaml
services:
  registry:
    image: mcpmesh/registry:0.7
    ports:
      - "8000:8000"
    environment:
      DATABASE_URL: postgresql://postgres:postgres@postgres:5432/mcpmesh

  job-agent:
    image: mcpmesh/python-runtime:0.7
    environment:
      MCP_MESH_REGISTRY_URL: http://registry:8000
      AGENT_NAME: job-agent
    volumes:
      - ./agents/job_agent:/app

  # ... other agents

  postgres:
    image: postgres:15

  tempo:
    image: grafana/tempo:latest

  grafana:
    image: grafana/grafana:latest
#+end_src

Same agent code. Real multi-container networking. PostgreSQL for state.

*** Kubernetes: Production Scale

*Helm charts:*

#+begin_src bash
# Install core infrastructure
helm install mcp-mesh oci://ghcr.io/dhyansraj/mcp-mesh/mcp-mesh-core

# Deploy agents
helm install job-agent oci://ghcr.io/dhyansraj/mcp-mesh/mcp-mesh-agent \
  --set agent.name=job-agent \
  --set agent.source.git.url=https://github.com/myorg/agents \
  --set agent.source.git.path=job_agent
#+end_src

What changes:
- Registry runs as a Deployment with PostgreSQL
- Agents are separate Deployments
- Ingress for external access
- HPA for auto-scaling
- PersistentVolumeClaims for state

What doesn't change:
- Agent code
- Capability declarations
- Dependency injection
- Mesh discovery

*** Environment Parity

| Aspect | Local | Docker | Kubernetes |
|--------|-------|--------|------------|
| Agent code | Same | Same | Same |
| Discovery | localhost | docker network | k8s service |
| Database | SQLite | PostgreSQL | PostgreSQL |
| Tracing | Optional | Tempo | Tempo/Jaeger |
| Scaling | Manual | Manual | HPA |

The mesh abstracts environment differences. Agents don't know or care where they're running.

*** Configuration by Environment

#+begin_src python
# Agent code - no environment awareness
@mesh.tool(capability="job_search")
async def search_jobs(query: str) -> list[Job]:
    ...
#+end_src

Environment config via variables:

#+begin_src bash
# Development
MCP_MESH_REGISTRY_URL=http://localhost:8000

# Docker
MCP_MESH_REGISTRY_URL=http://registry:8000

# Kubernetes
MCP_MESH_REGISTRY_URL=http://mcp-mesh-registry.mesh-system.svc:8000
#+end_src

Agents read registry URL from environment. Everything else is discovered.

*** GKE-Specific Notes

*1. Ingress Configuration*

#+begin_src yaml
# values.yaml
ingress:
  enabled: true
  className: gce
  annotations:
    kubernetes.io/ingress.global-static-ip-name: mcp-mesh-ip
  hosts:
    - host: mesh.example.com
      paths:
        - path: /
          pathType: Prefix
#+end_src

*2. Workload Identity*

#+begin_src yaml
serviceAccount:
  annotations:
    iam.gke.io/gcp-service-account: mcp-mesh@project.iam.gserviceaccount.com
#+end_src

*3. Cloud SQL*

#+begin_src yaml
postgresql:
  enabled: false  # Use Cloud SQL instead
  external:
    host: /cloudsql/project:region:instance
#+end_src

*** CI/CD Pipeline

#+begin_src yaml
# .github/workflows/deploy.yml
jobs:
  deploy:
    steps:
      - name: Test locally
        run: |
          meshctl start &
          meshctl start agents/job_agent.py &
          meshctl call job_search '{"query": "test"}'
          meshctl stop-all

      - name: Build and push
        run: |
          docker build -t gcr.io/project/job-agent:$SHA agents/job_agent
          docker push gcr.io/project/job-agent:$SHA

      - name: Deploy to GKE
        run: |
          helm upgrade job-agent oci://ghcr.io/.../mcp-mesh-agent \
            --set image.tag=$SHA
#+end_src

Test locally, deploy with confidence.

*** What's Next

Same code, any environment. But what about data? Each agent needs its own namespace.

ðŸ‘‰ [Next: Schema isolation â€” one database, autonomous agents]


* Article 15: Schema Isolation
:PROPERTIES:
:EXPORT_TITLE: Schema Isolation â€” One Database, Autonomous Agents
:EXPORT_TAGS: database, postgresql, architecture, data
:END:

** Meta

- *Goal*: Show how agents maintain data autonomy in shared infrastructure
- *Tone*: Architecture-focused
- *Length*: ~1100 words

** Article Draft

*** Introduction

Microservices purists say: one database per service.

Reality says: that's expensive and complex.

MCP Mesh uses PostgreSQL schemas for data isolation â€” one database, autonomous namespaces.

*** The Pattern

Each agent owns a schema:

#+begin_src sql
CREATE SCHEMA job_agent;
CREATE SCHEMA interview_agent;
CREATE SCHEMA user_agent;
CREATE SCHEMA application_agent;
#+end_src

Agent tables live in their schema:

#+begin_src sql
CREATE TABLE job_agent.jobs (...);
CREATE TABLE job_agent.categories (...);

CREATE TABLE interview_agent.interviews (...);
CREATE TABLE interview_agent.questions (...);
#+end_src

*** SQLAlchemy Configuration

#+begin_src python
from sqlalchemy import Column, String
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class Job(Base):
    __tablename__ = "jobs"
    __table_args__ = {"schema": "job_agent"}

    id = Column(String, primary_key=True)
    title = Column(String)
    # ...
#+end_src

Agent's database URL includes schema:

#+begin_src python
# Each agent connects with schema in search path
engine = create_engine(
    "postgresql://user:pass@host/db",
    connect_args={"options": "-csearch_path=job_agent,public"}
)
#+end_src

*** Benefits

*1. Namespace Isolation*

Agents can't accidentally read/write each other's data:

#+begin_src python
# job_agent can only see job_agent.* tables
# Even if it tried: SELECT * FROM interview_agent.interviews
# Result: permission denied
#+end_src

*2. Independent Migrations*

Each agent manages its own schema:

#+begin_src bash
# Job agent migrations
alembic -c job_agent/alembic.ini upgrade head

# Interview agent migrations (independent)
alembic -c interview_agent/alembic.ini upgrade head
#+end_src

No coordination needed. Agents evolve independently.

*3. Clear Ownership*

Looking at the database:

#+begin_src sql
\dn
        List of schemas
       Name        | Owner
-------------------+-------
 job_agent         | job_agent_user
 interview_agent   | interview_agent_user
 user_agent        | user_agent_user
 public            | postgres
#+end_src

Obvious who owns what.

*4. Shared Infrastructure*

One PostgreSQL instance. One backup strategy. One monitoring setup.

Cost effective. Operationally simple.

*** Cross-Agent Data Access

Agents should NOT query each other's tables directly.

Instead, use capabilities:

#+begin_src python
# Wrong: Direct database access
async def get_job_for_interview(job_id: str):
    # DON'T DO THIS
    result = await db.execute(
        "SELECT * FROM job_agent.jobs WHERE id = :id",
        {"id": job_id}
    )

# Right: Use capability
@mesh.tool(
    capability="interview_setup",
    dependencies=[{"capability": "job_details"}]
)
async def setup_interview(
    job_id: str,
    job_details: McpMeshAgent = None
):
    job = await job_details(job_id=job_id)  # Proper API call
#+end_src

This maintains:
- Data encapsulation
- Schema evolution independence
- Clear contracts between agents

*** PostgreSQL Permissions

Lock it down:

#+begin_src sql
-- Create users per agent
CREATE USER job_agent_user WITH PASSWORD 'xxx';
CREATE USER interview_agent_user WITH PASSWORD 'yyy';

-- Grant schema access
GRANT USAGE ON SCHEMA job_agent TO job_agent_user;
GRANT ALL ON ALL TABLES IN SCHEMA job_agent TO job_agent_user;

-- Revoke cross-schema access
REVOKE ALL ON SCHEMA interview_agent FROM job_agent_user;
#+end_src

Even if code has a bug, database permissions prevent cross-agent data access.

*** Migration Strategy

*Initial Setup:*

#+begin_src sql
-- Run once per agent
CREATE SCHEMA IF NOT EXISTS {{ agent_name }};
#+end_src

*Ongoing Migrations:*

Each agent has its own Alembic config:

#+begin_src ini
# job_agent/alembic.ini
[alembic]
script_location = job_agent/migrations
sqlalchemy.url = postgresql://job_agent_user:xxx@host/db

[alembic:exclude]
schemas = interview_agent,user_agent,application_agent
#+end_src

Migrations are scoped to the agent's schema.

*** When to Use Separate Databases

Schema isolation works for most cases. Consider separate databases when:

- Regulatory requirements (data residency)
- Extreme scale (millions of rows per agent)
- Different database engines needed (PostgreSQL + Redis + MongoDB)
- Multi-tenant SaaS (tenant per database)

For typical multi-agent systems, schema isolation is the sweet spot.

*** Summary

| Approach | Pros | Cons |
|----------|------|------|
| Database per agent | Full isolation | Expensive, complex |
| Shared database, no isolation | Simple | Data governance nightmare |
| Schema isolation | Balanced | Requires discipline |

MCP Mesh recommends schema isolation: one database, clear boundaries, autonomous agents.

*** Series Conclusion

This completes the MCP Mesh article series:

- *Week 1*: The case study and MCP vs REST
- *Week 2*: The paradigm shift (capabilities, registry, evolution)
- *Week 3*: Building blocks (DI, heartbeats, tags, LLMs)
- *Week 4*: AI-native patterns and production deployment

MCP Mesh represents a new approach to distributed AI systems: agents that self-organize, capabilities that self-describe, and infrastructure that gets out of the way.

Try it: [[https://github.com/dhyansraj/mcp-mesh][github.com/dhyansraj/mcp-mesh]]

ðŸ‘‰ [Back to: Part 0 â€” The Case Study]


* Revision Notes

** Article 9-12 (AI-Native)
- [ ] Add sequence diagrams for agent collaboration
- [ ] Include actual LLM response examples
- [ ] Show Pydantic validation error messages
- [ ] Add template testing examples

** Article 13-15 (Production)
- [ ] Add Grafana dashboard screenshots
- [ ] Include actual Helm values examples
- [ ] Add GKE cost comparison
- [ ] Document backup/restore procedures

** Publishing Checklist
- [ ] Ensure Week 3 articles are live
- [ ] Add series index page
- [ ] Create cover images for each article
- [ ] Schedule: 2 articles per week
- [ ] Final article should link back to Part 0 for series loop
