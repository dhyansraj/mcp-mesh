#+TITLE: Week 3 Articles - Building Blocks
#+AUTHOR: MCP Mesh Team
#+DATE: 2024-12
#+OPTIONS: toc:2 num:t

* Overview

Week 3 explains the mechanics:
- Article 5: Dependency injection for distributed systems
- Article 6: Heartbeats and topology
- Article 7: Tags and versions for routing
- Article 8: LLMs as first-class capabilities

These are the "how it works" articles that give readers confidence in the implementation.

* Article 5: Dependency Injection for Distributed Systems
:PROPERTIES:
:EXPORT_TITLE: Dependency Injection for Distributed Systems
:EXPORT_TAGS: dependency-injection, microservices, mcp, python
:END:

** Meta

- *Goal*: Show how DI works across network boundaries in MCP Mesh
- *Tone*: Technical, code-heavy
- *Length*: ~1500 words
- *CTA*: "Next: How heartbeats keep the mesh alive"

** Article Draft

*** Introduction

Dependency injection in a monolith is simple:

#+begin_src python
class OrderService:
    def __init__(self, user_repo: UserRepository, payment: PaymentService):
        self.user_repo = user_repo
        self.payment = payment
#+end_src

Your DI container wires it up. Clean, testable, well-understood.

But what happens when =PaymentService= is on a different server? A different cluster? A different continent?

MCP Mesh brings dependency injection to distributed systems ‚Äî with the same developer experience.

*** The Problem with Remote Dependencies

Traditional microservices handle remote dependencies imperatively:

#+begin_src python
import httpx

class OrderService:
    def __init__(self):
        self.payment_url = os.getenv("PAYMENT_SERVICE_URL")

    async def create_order(self, order: Order):
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.payment_url}/charge",
                json={"amount": order.total}
            )
            if response.status_code != 200:
                raise PaymentError(response.text)
            ...
#+end_src

Problems:
- URL hardcoded (or in config)
- HTTP client boilerplate
- Error handling scattered
- No type safety on the remote call
- Testing requires mocking HTTP

*** MCP Mesh: Declarative Dependencies

#+begin_src python
@mesh.tool(
    capability="create_order",
    dependencies=[
        {"capability": "charge_payment"},
        {"capability": "get_user"}
    ]
)
async def create_order(
    order: Order,
    charge_payment: McpMeshAgent = None,
    get_user: McpMeshAgent = None
) -> OrderResult:

    user = await get_user(user_id=order.user_id)
    payment = await charge_payment(
        amount=order.total,
        user_id=order.user_id
    )

    return OrderResult(order_id=order.id, payment_id=payment.id)
#+end_src

What's different:
- Dependencies declared, not discovered at runtime
- No URLs, no HTTP clients
- Type-safe calls (mesh knows the schema)
- Testable (inject mocks directly)
- Mesh handles resolution, transport, errors

*** How It Works

*1. Declaration*

The =dependencies= parameter lists what capabilities this tool needs:

#+begin_src python
dependencies=[
    {"capability": "charge_payment"},
    {"capability": "get_user", "tags": ["+v2"]},
    {"capability": "send_notification", "version": ">=1.5.0"}
]
#+end_src

Each dependency can specify:
- =capability=: Required capability name
- =tags=: Tag constraints (=+required=, =-excluded=, =preferred=)
- =version=: Semantic version constraint

*2. Resolution*

When the agent starts:
1. Sends heartbeat with dependency list to registry
2. Registry resolves each capability to an endpoint
3. Registry returns topology with resolved dependencies
4. Mesh runtime creates =McpMeshAgent= proxies

*3. Injection*

Parameters matching dependency names get injected:

#+begin_src python
async def create_order(
    order: Order,
    charge_payment: McpMeshAgent = None,  # Matches "charge_payment" capability
    get_user: McpMeshAgent = None         # Matches "get_user" capability
):
#+end_src

The =McpMeshAgent= is a callable proxy. When you call it, it:
- Makes an MCP call to the resolved endpoint
- Handles transport (HTTP/SSE/stdio)
- Validates request/response schemas
- Handles errors and retries

*4. Invocation*

#+begin_src python
result = await charge_payment(amount=100, user_id="user-123")
#+end_src

This becomes an MCP tool call to the payment agent. The mesh handles serialization, transport, and response parsing.

*** Graceful Degradation

What if a dependency isn't available?

#+begin_src python
@mesh.tool(
    capability="create_order",
    dependencies=[
        {"capability": "charge_payment"},
        {"capability": "fraud_check"}  # Optional enhancement
    ]
)
async def create_order(
    order: Order,
    charge_payment: McpMeshAgent = None,
    fraud_check: McpMeshAgent = None
) -> OrderResult:

    # Required dependency - will raise if missing
    if not charge_payment:
        raise DependencyError("Payment service unavailable")

    # Optional dependency - degrade gracefully
    if fraud_check:
        fraud_result = await fraud_check(order=order)
        if fraud_result.is_fraud:
            raise FraudDetectedError()

    payment = await charge_payment(amount=order.total)
    return OrderResult(...)
#+end_src

Pattern: Check if dependency is =None= before using. Required dependencies raise early. Optional dependencies degrade gracefully.

*** Testing with Injected Dependencies

Unit testing becomes trivial:

#+begin_src python
import pytest
from unittest.mock import AsyncMock

async def test_create_order():
    # Mock dependencies
    mock_payment = AsyncMock(return_value={"id": "pay-123"})
    mock_user = AsyncMock(return_value={"id": "user-123", "email": "test@example.com"})

    # Call with injected mocks
    result = await create_order(
        order=Order(id="order-1", total=100, user_id="user-123"),
        charge_payment=mock_payment,
        get_user=mock_user
    )

    # Verify
    assert result.payment_id == "pay-123"
    mock_payment.assert_called_with(amount=100, user_id="user-123")
#+end_src

No HTTP mocking. No container setup. Just function calls with injected dependencies.

*** Integration Testing

For integration tests, run actual agents:

#+begin_src python
@pytest.fixture
async def mesh_agents():
    # Start agents in test mode
    await meshctl.start("payment_agent.py", port=9001)
    await meshctl.start("user_agent.py", port=9002)
    await meshctl.start("order_agent.py", port=9003)

    yield

    # Cleanup
    await meshctl.stop_all()

async def test_create_order_integration(mesh_agents):
    result = await meshctl.call("create_order", {
        "order": {"id": "test-1", "total": 100, "user_id": "user-1"}
    })

    assert result["success"]
    assert "payment_id" in result
#+end_src

*** Comparing DI Approaches

| Aspect | Local DI | HTTP Calls | MCP Mesh DI |
|--------|----------|------------|-------------|
| Declaration | Constructor | Config/code | Decorator |
| Resolution | Container | Manual | Mesh |
| Type safety | Yes | No | Yes |
| Transport | N/A | Manual | Automatic |
| Testing | Mock objects | Mock HTTP | Mock objects |
| Graceful degradation | N/A | Try/catch | None check |

MCP Mesh brings the ergonomics of local DI to distributed systems.

*** Advanced: Filter Modes

When multiple providers match a capability:

#+begin_src python
dependencies=[
    {"capability": "translate", "filter_mode": "best_match"}
]
#+end_src

Filter modes:
- =all=: Return all matching providers (for fan-out)
- =best_match=: Return single best match (default)
- =*=: Wildcard ‚Äî match any single provider

*** What's Next

Dependency injection relies on the mesh knowing what's available. That knowledge comes from heartbeats.

Next article: How the heartbeat system keeps the mesh topology current.

üëâ [Next: Heartbeats and topology ‚Äî how agents stay connected]


* Article 6: Heartbeats and Topology
:PROPERTIES:
:EXPORT_TITLE: Heartbeats and Topology ‚Äî How Agents Stay Connected
:EXPORT_TAGS: distributed-systems, health-checks, mcp, reliability
:END:

** Meta

- *Goal*: Explain the heartbeat system and topology propagation
- *Tone*: Technical, with timing details
- *Length*: ~1300 words
- *CTA*: "Next: Tags and versions for routing"

** Article Draft

*** Introduction

In a distributed system, the hardest problem isn't sending messages. It's knowing who's alive to receive them.

MCP Mesh solves this with a dual-heartbeat system: lightweight presence checks and detailed topology updates. Here's how it works.

*** The Dual-Heartbeat System

| Type | Method | Frequency | Payload Size | Purpose |
|------|--------|-----------|--------------|---------|
| Presence | HEAD | ~5 seconds | ~200 bytes | "I'm alive" |
| Registration | POST | On change | ~2KB | Full capability data |

Most of the time, agents send HEAD requests ‚Äî just enough to say "still here." Full POST registrations only happen when something changes.

*** HEAD Heartbeats: Presence

Every 5 seconds (configurable), each agent sends:

#+begin_src http
HEAD /heartbeat HTTP/1.1
Host: registry:8000
X-Agent-ID: agent-abc-123
X-Topology-Version: 42
#+end_src

Registry responds:

#+begin_src http
HTTP/1.1 200 OK
X-Topology-Version: 42
#+end_src

If versions match, nothing more happens. The agent is alive, topology is current.

If versions differ:

#+begin_src http
HTTP/1.1 200 OK
X-Topology-Version: 43
X-Topology-Changed: true
#+end_src

The agent then fetches the new topology or sends a full POST to get updated dependencies.

*** POST Heartbeats: Registration

Full registration happens when:
- Agent first starts
- Agent's capabilities change
- Topology version mismatch detected

#+begin_src http
POST /heartbeat HTTP/1.1
Host: registry:8000
Content-Type: application/json

{
  "agent_id": "agent-abc-123",
  "agent_name": "order-processor",
  "version": "1.2.0",
  "endpoint": "http://order-processor:9000",
  "capabilities": [
    {
      "name": "create_order",
      "version": "1.0.0",
      "tags": ["orders", "production"],
      "schema": { ... }
    }
  ],
  "dependencies": [
    {"capability": "charge_payment"},
    {"capability": "get_user"}
  ]
}
#+end_src

Registry responds with resolved topology:

#+begin_src json
{
  "topology_version": 43,
  "dependencies": {
    "charge_payment": {
      "agent_id": "payment-agent-456",
      "endpoint": "http://payment-agent:9001"
    },
    "get_user": {
      "agent_id": "user-agent-789",
      "endpoint": "http://user-agent:9002"
    }
  }
}
#+end_src

*** Failure Detection

What happens when an agent dies?

#+begin_src
Timeline (5-second heartbeat interval):

00:00  Agent A: heartbeat ‚úì
00:05  Agent A: heartbeat ‚úì
00:10  Agent A: heartbeat ‚úì
00:12  Agent A crashes
00:15  Registry: expected heartbeat from A (missed #1)
00:20  Registry: expected heartbeat from A (missed #2)
00:25  Registry: expected heartbeat from A (missed #3)
00:30  Registry: expected heartbeat from A (missed #4)
00:30  Registry: Agent A marked UNHEALTHY
00:30  Registry: Topology version incremented (43 ‚Üí 44)
00:35  Agent B: heartbeat (gets topology v44, Agent A removed)
00:35  Agent B: dependency on Agent A ‚Üí None
#+end_src

Configuration:
- Heartbeat interval: 5 seconds (default)
- Miss threshold: 4 heartbeats (default)
- Detection time: ~20 seconds worst case

*** Topology Propagation

When topology changes (agent joins, leaves, or updates):

1. Registry increments =topology_version=
2. Next heartbeat from any agent includes old version
3. Registry detects version mismatch
4. Agent receives new topology in response

This is *pull-based* propagation. Agents discover changes on their next heartbeat, not via push.

Advantages:
- No websocket connections to maintain
- Works through firewalls and load balancers
- Scales to thousands of agents
- Agents control their own refresh rate

Tradeoff:
- Propagation delay up to one heartbeat interval
- For 5-second heartbeats, worst case is ~5 seconds to learn of change

*** Health States

Agents can be in three states:

| State | Meaning | Routing |
|-------|---------|---------|
| HEALTHY | Recent heartbeat received | Included in routing |
| UNHEALTHY | Missed heartbeat threshold | Excluded from routing |
| UNKNOWN | Never registered | Not in topology |

State transitions:

#+begin_src
              heartbeat
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                        ‚îÇ
    ‚ñº                        ‚îÇ
UNKNOWN ‚îÄ‚îÄregister‚îÄ‚îÄ‚ñ∫ HEALTHY ‚óÑ‚îÄ‚îÄheartbeat‚îÄ‚îÄ‚îê
                         ‚îÇ                   ‚îÇ
                    miss threshold           ‚îÇ
                         ‚îÇ                   ‚îÇ
                         ‚ñº                   ‚îÇ
                     UNHEALTHY ‚îÄ‚îÄheartbeat‚îÄ‚îÄ‚îÄ‚îò
#+end_src

*** Configuring Heartbeats

For different environments:

#+begin_src python
# Development: Fast detection
@mesh.agent(
    name="my-agent",
    heartbeat_interval=2,  # 2 seconds
    heartbeat_timeout=8    # 4 missed = 8 seconds
)

# Production: Balance detection vs. load
@mesh.agent(
    name="my-agent",
    heartbeat_interval=5,   # 5 seconds
    heartbeat_timeout=20    # 4 missed = 20 seconds
)

# Batch jobs: Relaxed timing
@mesh.agent(
    name="batch-processor",
    heartbeat_interval=30,  # 30 seconds
    heartbeat_timeout=120   # 4 missed = 2 minutes
)
#+end_src

*** Registry Scalability

The registry handles heartbeats efficiently:

| Agents | HEAD requests/sec | POST requests/sec | Registry CPU |
|--------|-------------------|-------------------|--------------|
| 10 | 2 | ~0.1 | <1% |
| 100 | 20 | ~1 | <5% |
| 1000 | 200 | ~10 | ~15% |

HEAD requests are tiny and fast. POST requests are rare (only on change). The registry is not a bottleneck.

*** Heartbeat vs. Health Checks

MCP Mesh heartbeats are different from Kubernetes health checks:

| Aspect | K8s Health Checks | MCP Mesh Heartbeats |
|--------|-------------------|---------------------|
| Direction | Kubelet ‚Üí Pod | Agent ‚Üí Registry |
| Purpose | Container liveness | Capability availability |
| Granularity | Pod level | Capability level |
| Topology | No | Yes (includes dependencies) |

They're complementary. Use K8s health checks for container lifecycle. Use mesh heartbeats for capability discovery.

*** What's Next

Heartbeats keep the mesh aware of who's alive. Tags and versions determine *which* alive agent handles a request.

Next article: Routing with tags and versions ‚Äî no service mesh sidecars required.

üëâ [Next: Tags and versions ‚Äî routing without service mesh sidecars]


* Article 7: Tags and Versions
:PROPERTIES:
:EXPORT_TITLE: Tags and Versions ‚Äî Routing Without Service Mesh Sidecars
:EXPORT_TAGS: routing, microservices, mcp, service-mesh
:END:

** Meta

- *Goal*: Show how tags and versions enable sophisticated routing
- *Tone*: Practical with examples
- *Length*: ~1400 words
- *CTA*: "Next: LLMs as capabilities"

** Article Draft

*** Introduction

Traditional service meshes use sidecars for routing:

#+begin_src
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Service A           ‚îÇ     ‚îÇ Service B           ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ     ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ App Container   ‚îÇ ‚îÇ     ‚îÇ ‚îÇ App Container   ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ     ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ          ‚îÇ          ‚îÇ     ‚îÇ          ‚îÇ          ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ     ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ Envoy Sidecar   ‚îÇ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÇ Envoy Sidecar   ‚îÇ ‚îÇ
‚îÇ ‚îÇ (~50MB RAM)     ‚îÇ ‚îÇ     ‚îÇ ‚îÇ (~50MB RAM)     ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ     ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
#+end_src

MCP Mesh puts routing logic in the registry and capability metadata. No sidecars. No per-pod overhead.

*** Tags: Flexible Categorization

Tags are arbitrary labels on capabilities:

#+begin_src python
@mesh.tool(
    capability="process_payment",
    tags=["payments", "production", "us-east", "stripe", "v2"]
)
async def process_payment(amount: float) -> PaymentResult:
    ...
#+end_src

Tags have no predefined meaning. Use them for:
- Environment: =production=, =staging=, =development=
- Region: =us-east=, =eu-west=, =ap-south=
- Provider: =stripe=, =paypal=, =braintree=
- Version: =v1=, =v2=, =legacy=
- Features: =pci-compliant=, =3ds-enabled=

*** Tag Operators

When declaring dependencies, use operators:

| Operator | Meaning | Example |
|----------|---------|---------|
| =+tag= | MUST have tag | =+production= |
| =-tag= | MUST NOT have tag | =-deprecated= |
| =tag= | PREFER tag (soft) | =us-east= |

#+begin_src python
dependencies=[
    {
        "capability": "process_payment",
        "tags": ["+production", "-deprecated", "us-east", "stripe"]
    }
]
#+end_src

This says:
- MUST be production
- MUST NOT be deprecated
- PREFER us-east (but accept others)
- PREFER stripe (but accept others)

*** Tag Resolution

When multiple providers match, the mesh scores them:

#+begin_src
Request: +production, -deprecated, us-east, stripe

Provider A: [production, us-east, stripe]        ‚Üí Score: 3/3 soft matches ‚úì
Provider B: [production, eu-west, paypal]        ‚Üí Score: 1/3 soft matches
Provider C: [staging, us-east, stripe]           ‚Üí REJECTED (missing +production)
Provider D: [production, deprecated, us-east]    ‚Üí REJECTED (has -deprecated)
#+end_src

Provider A wins. If A becomes unhealthy, B takes over automatically.

*** Version Constraints

Capabilities have semantic versions:

#+begin_src python
@mesh.tool(
    capability="user_api",
    version="2.3.1"
)
#+end_src

Consumers specify constraints:

#+begin_src python
dependencies=[
    {"capability": "user_api", "version": ">=2.0.0, <3.0.0"}
]
#+end_src

Standard semver operators:
- =>= 2.0.0=: At least version 2.0.0
- =<3.0.0=: Less than 3.0.0
- =~=2.3.0=: Compatible with 2.3.x
- ===2.3.1=: Exactly 2.3.1

*** Routing Scenarios

*Scenario 1: Blue-Green Deployment*

#+begin_src python
# Blue (current)
@mesh.tool(capability="api", tags=["production", "blue"])

# Green (new)
@mesh.tool(capability="api", tags=["production", "green"])
#+end_src

Route to blue:
#+begin_src python
dependencies=[{"capability": "api", "tags": ["+production", "+blue"]}]
#+end_src

Switch to green:
#+begin_src python
dependencies=[{"capability": "api", "tags": ["+production", "+green"]}]
#+end_src

*Scenario 2: Canary Release*

#+begin_src python
# Stable (90% traffic)
@mesh.tool(capability="api", tags=["production", "stable"])

# Canary (10% traffic) - same capability, different instance
@mesh.tool(capability="api", tags=["production", "canary"])
#+end_src

For canary routing by percentage, use filter modes:
#+begin_src python
dependencies=[
    {"capability": "api", "tags": ["+production"], "filter_mode": "best_match"}
]
#+end_src

The mesh load-balances across matching providers. Adjust instance counts for traffic split.

*Scenario 3: Regional Affinity*

#+begin_src python
# US East
@mesh.tool(capability="cache", tags=["production", "us-east"])

# EU West
@mesh.tool(capability="cache", tags=["production", "eu-west"])
#+end_src

Agents prefer local region:
#+begin_src python
# Agent in US East
dependencies=[
    {"capability": "cache", "tags": ["+production", "us-east"]}
]
#+end_src

If us-east is down, eu-west is still a valid match (soft preference).

*Scenario 4: Feature Flags*

#+begin_src python
# Standard payment
@mesh.tool(capability="payment", tags=["production"])

# Payment with 3DS
@mesh.tool(capability="payment", tags=["production", "3ds"])
#+end_src

Consumers opt into features:
#+begin_src python
# Want 3DS
dependencies=[{"capability": "payment", "tags": ["+production", "+3ds"]}]

# Don't care about 3DS
dependencies=[{"capability": "payment", "tags": ["+production"]}]
#+end_src

*** Combining Tags and Versions

Full routing power:

#+begin_src python
dependencies=[
    {
        "capability": "ml_model",
        "version": ">=2.0.0",
        "tags": ["+production", "-experimental", "gpu", "us-east"]
    }
]
#+end_src

This finds:
- =ml_model= capability
- Version 2.0.0 or higher
- MUST be production
- MUST NOT be experimental
- PREFER gpu-enabled
- PREFER us-east

*** No Sidecars Required

Traditional service meshes need sidecars because routing rules are external to the application. They intercept traffic and apply rules.

MCP Mesh embeds routing in:
1. *Capability metadata* (provider declares tags/version)
2. *Dependency declarations* (consumer specifies constraints)
3. *Registry resolution* (mesh matches providers to consumers)

No interception. No proxies. No per-pod overhead.

| Aspect | Sidecar Mesh | MCP Mesh |
|--------|--------------|----------|
| Memory overhead | ~50MB per pod | 0 |
| Latency overhead | ~1-5ms per hop | 0 |
| Routing logic | External rules | In code |
| Deployment complexity | High | Low |
| Debugging | Hard (proxy logs) | Easy (application logs) |

*** What's Next

Tags and versions route to the right capability. But what about LLMs? They're capabilities too.

Next article: How LLM providers become first-class citizens in the mesh.

üëâ [Next: LLMs as capabilities ‚Äî AI providers in the mesh]


* Article 8: LLMs as Capabilities
:PROPERTIES:
:EXPORT_TITLE: LLMs as Capabilities ‚Äî AI Providers in the Mesh
:EXPORT_TAGS: llm, ai, mcp, openai, anthropic
:END:

** Meta

- *Goal*: Show how LLMs are just another capability, not special-cased
- *Tone*: Exciting, showing the power
- *Length*: ~1400 words
- *CTA*: "Week 3 complete ‚Äî next week: AI-native patterns"

** Article Draft

*** Introduction

Most frameworks treat LLMs as special:
- Hardcoded client libraries
- Config-driven provider selection
- Manual failover logic
- Different APIs for different providers

MCP Mesh treats LLMs as capabilities. They register, get discovered, and failover like any other service in the mesh.

This changes everything.

*** LLM Provider as a Capability

#+begin_src python
from mcp_mesh import mesh

@mesh.agent(name="claude-provider", port=9010)
@mesh.llm_provider(
    model="anthropic/claude-sonnet-4-20250514",
    capability="llm",
    tags=["claude", "production", "fast"]
)
async def claude_provider():
    pass  # Framework handles the implementation
#+end_src

That's a complete LLM provider. The =@mesh.llm_provider= decorator:
- Registers =llm= capability with the mesh
- Handles Anthropic API calls internally
- Exposes standard MCP tool interface
- Streams responses properly

*** Multiple Providers

Deploy multiple providers with different tags:

#+begin_src python
# Claude provider
@mesh.llm_provider(
    model="anthropic/claude-sonnet-4-20250514",
    capability="llm",
    tags=["claude", "production"]
)

# OpenAI provider
@mesh.llm_provider(
    model="openai/gpt-4o",
    capability="llm",
    tags=["openai", "production"]
)

# Local Ollama provider
@mesh.llm_provider(
    model="ollama/llama3",
    capability="llm",
    tags=["local", "development"]
)
#+end_src

All provide the =llm= capability. Tags differentiate them.

*** Consuming LLM Capabilities

Agents consume LLMs like any other dependency:

#+begin_src python
@mesh.tool(
    capability="analyze_resume",
    dependencies=[
        {"capability": "llm", "tags": ["+claude"]}
    ]
)
async def analyze_resume(
    resume_text: str,
    llm: McpMeshAgent = None
) -> AnalysisResult:

    prompt = f"Analyze this resume:\n\n{resume_text}"
    response = await llm(prompt)

    return AnalysisResult(summary=response)
#+end_src

The agent doesn't know:
- Which Claude model is being used
- What API key is configured
- How to handle rate limits
- How streaming works

It just calls =llm()= with a prompt.

*** The @mesh.llm Decorator

For more control, use =@mesh.llm= on your tools:

#+begin_src python
@mesh.tool(capability="interview_question")
@mesh.llm(
    provider={"capability": "llm", "tags": ["+openai"]},
    system_prompt="You are a technical interviewer...",
    max_iterations=1
)
async def generate_question(
    context: InterviewContext,
    llm: MeshLlmAgent = None
) -> InterviewQuestion:

    result = await llm(
        f"Generate a question about {context.topic}"
    )

    return result
#+end_src

The =@mesh.llm= decorator:
- Selects provider based on tags
- Injects system prompt
- Handles multi-turn if needed
- Returns typed response

*** Automatic Failover

Here's where it gets interesting.

#+begin_src
Topology at t=0:
- claude-provider: HEALTHY
- openai-provider: HEALTHY

Agent's dependency: {"capability": "llm", "tags": ["claude"]}
Resolved to: claude-provider
#+end_src

Claude's API goes down:

#+begin_src
Topology at t=20s:
- claude-provider: UNHEALTHY (missed heartbeats)
- openai-provider: HEALTHY

Agent's dependency: {"capability": "llm", "tags": ["claude"]}
Resolved to: openai-provider (fallback - "claude" tag was soft preference)
#+end_src

The agent's next LLM call automatically routes to OpenAI. No code change. No config update. No restart.

When Claude recovers:

#+begin_src
Topology at t=60s:
- claude-provider: HEALTHY
- openai-provider: HEALTHY

Agent's dependency: {"capability": "llm", "tags": ["claude"]}
Resolved to: claude-provider (preferred match restored)
#+end_src

Traffic returns to Claude. Again, automatically.

*** Structured Output with Pydantic

LLM responses can be typed:

#+begin_src python
class ResumeAnalysis(BaseModel):
    name: str
    experience_years: int
    skills: list[str]
    summary: str

@mesh.tool(capability="analyze_resume")
@mesh.llm(provider={"capability": "llm"})
async def analyze_resume(
    resume_text: str,
    llm: MeshLlmAgent = None
) -> ResumeAnalysis:  # Return type enforced

    result = await llm(f"Analyze: {resume_text}")
    return result  # Automatically parsed to ResumeAnalysis
#+end_src

The mesh:
1. Sends schema to LLM provider
2. Provider requests JSON output
3. Response parsed to Pydantic model
4. Validation errors caught early

*** Context Injection with Jinja2

Dynamic prompts with context:

#+begin_src python
@mesh.llm(
    provider={"capability": "llm"},
    system_prompt="file://prompts/interviewer.jinja2",
    context_param="ctx"
)
async def generate_question(
    ctx: InterviewContext,
    llm: MeshLlmAgent = None
) -> InterviewQuestion:
    ...
#+end_src

Template (=prompts/interviewer.jinja2=):

#+begin_src jinja2
You are interviewing for: {{ ctx.job_title }}

Requirements:
{% for req in ctx.requirements %}
- {{ req }}
{% endfor %}

Candidate background:
{{ ctx.resume_summary }}

Questions asked so far: {{ ctx.questions_asked }}
Time remaining: {{ ctx.time_remaining }} minutes
#+end_src

Context is injected at runtime. Prompts stay maintainable.

*** Multi-Provider Strategies

*Strategy 1: Primary/Fallback*

#+begin_src python
dependencies=[
    {"capability": "llm", "tags": ["+production", "claude"]}
]
#+end_src

Prefers Claude, falls back to any production LLM.

*Strategy 2: Cost Optimization*

#+begin_src python
# Cheap tasks
dependencies=[{"capability": "llm", "tags": ["+cheap"]}]

# Important tasks
dependencies=[{"capability": "llm", "tags": ["+premium"]}]
#+end_src

Tag providers by cost tier. Route accordingly.

*Strategy 3: Capability-Specific*

#+begin_src python
# Vision tasks
dependencies=[{"capability": "llm", "tags": ["+vision"]}]

# Code tasks
dependencies=[{"capability": "llm", "tags": ["+code"]}]
#+end_src

Some models are better at certain tasks. Tags make routing explicit.

*** Why This Matters

Traditional LLM integration:

#+begin_src python
import anthropic
import openai

def call_llm(prompt):
    try:
        client = anthropic.Anthropic()
        return client.messages.create(...)
    except anthropic.APIError:
        # Fallback to OpenAI
        client = openai.OpenAI()
        return client.chat.completions.create(...)
#+end_src

Problems:
- Hardcoded fallback logic
- Different APIs per provider
- No automatic recovery
- Config changes require deploys

MCP Mesh LLM integration:

#+begin_src python
result = await llm(prompt)
#+end_src

- Provider resolved by mesh
- Failover automatic
- Recovery automatic
- Add providers without code changes

*** What's Next

Week 3 covered the building blocks:
- Dependency injection across the network
- Heartbeats for topology awareness
- Tags and versions for routing
- LLMs as first-class capabilities

Week 4 goes deeper into AI-native patterns: context injection, multi-turn conversations, structured outputs, and production deployment.

üëâ [Next week: AI-Native Patterns and Production]


* Revision Notes

** Article 5 (Dependency Injection)
- [ ] Add diagram showing injection flow
- [ ] Include actual McpMeshAgent API reference
- [ ] Add async context manager patterns

** Article 6 (Heartbeats)
- [ ] Add actual packet capture examples
- [ ] Include registry metrics screenshots
- [ ] Document PostgreSQL backend for HA

** Article 7 (Tags and Versions)
- [ ] Add meshctl commands for viewing tag resolution
- [ ] Include real routing decision logs
- [ ] Compare performance vs Envoy

** Article 8 (LLMs)
- [ ] Add LiteLLM model string reference
- [ ] Include streaming example
- [ ] Document token counting and cost tracking

** Publishing Checklist
- [ ] Ensure Week 2 articles are live
- [ ] Cross-reference to case study examples
- [ ] Add series navigation
- [ ] Schedule: One article every 2-3 days
