#+TITLE: Week 2 Articles - The Paradigm Shift
#+AUTHOR: MCP Mesh Team
#+DATE: 2024-12
#+OPTIONS: toc:2 num:t

* Overview

Week 2 establishes the core mental model shift:
- Article 2: Capabilities replace endpoints
- Article 3: Registry facilitates, doesn't orchestrate
- Article 4: Dynamic topology without restarts

These articles build on the "MCP vs REST" foundation and prepare readers for the building blocks in Week 3.

* Article 2: Capabilities, Not Endpoints
:PROPERTIES:
:EXPORT_TITLE: Capabilities, Not Endpoints â€” Loose Coupling Done Right
:EXPORT_TAGS: microservices, architecture, mcp, coupling
:END:

** Meta

- *Goal*: Explain capability-based service discovery as the core abstraction
- *Tone*: Conceptual but grounded in examples
- *Length*: ~1400 words
- *CTA*: "Next: How the registry makes this work"

** Article Draft

*** Introduction

In traditional microservices, services know each other by location:

#+begin_src
user-service â†’ http://order-service:8080/orders
order-service â†’ http://payment-service:8080/pay
payment-service â†’ http://notification-service:8080/notify
#+end_src

Every arrow is a hardcoded dependency. Change one URL, and you're updating configs across the system.

MCP Mesh inverts this. Services don't know locations. They know capabilities.

*** What is a Capability?

A capability is a named, versioned, tagged description of what a service can do.

#+begin_src python
@mesh.tool(
    capability="send_notification",
    version="1.2.0",
    tags=["email", "sms", "production"]
)
async def send_notification(
    recipient: str,
    message: str,
    channel: str = "email"
) -> NotificationResult:
    ...
#+end_src

This declares:
- *Name*: =send_notification=
- *Version*: =1.2.0=
- *Tags*: Supports email, sms, is production-ready
- *Schema*: Takes recipient, message, channel; returns NotificationResult

The service's location (host, port, URL) is *not* part of the capability. Location is an implementation detail the mesh handles.

*** Consuming Capabilities

When another service needs notifications, it declares the dependency:

#+begin_src python
@mesh.tool(
    capability="process_order",
    dependencies=[
        {"capability": "send_notification", "tags": ["+email"]}
    ]
)
async def process_order(
    order: Order,
    send_notification: McpMeshAgent = None  # Injected
) -> OrderResult:

    # Process the order...

    await send_notification(
        recipient=order.customer_email,
        message=f"Order {order.id} confirmed",
        channel="email"
    )

    return OrderResult(...)
#+end_src

The order processor doesn't know:
- Where the notification service runs
- What port it's on
- Whether it's one instance or ten
- Whether it's in the same cluster or a different region

It knows: "I need something that can =send_notification= with =+email= tag."

*** Tag Operators

Tags support operators for precise matching:

| Operator | Meaning | Example |
|----------|---------|---------|
| =+tag= | Must have this tag | =+production= |
| =-tag= | Must NOT have this tag | =-deprecated= |
| =tag= | Prefer this tag (soft match) | =email= |

#+begin_src python
dependencies=[
    # Must be production, must support email, prefer us-east
    {"capability": "send_notification", "tags": ["+production", "+email", "us-east"]}
]
#+end_src

This gives you routing control without hardcoding locations.

*** Version Constraints

Capabilities are versioned. Consumers can specify constraints:

#+begin_src python
dependencies=[
    {"capability": "user_profile", "version": ">=2.0.0, <3.0.0"}
]
#+end_src

The mesh routes to a compatible provider. If none exists, you get a clear error â€” not a runtime surprise when the API doesn't match.

*** Multiple Providers

What if two services provide the same capability?

#+begin_src
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Email Notifier      â”‚       â”‚ SMS Notifier        â”‚
â”‚ capability:         â”‚       â”‚ capability:         â”‚
â”‚   send_notification â”‚       â”‚   send_notification â”‚
â”‚ tags: [email]       â”‚       â”‚ tags: [sms]         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#+end_src

Both provide =send_notification=. Tags differentiate them.

A consumer asking for =+email= gets Email Notifier. A consumer asking for =+sms= gets SMS Notifier. A consumer asking for just =send_notification= gets whichever is healthiest.

*** The Loose Coupling Payoff

*Adding a new provider*

You deploy a new SMS Notifier in a different region:

#+begin_src python
@mesh.tool(
    capability="send_notification",
    tags=["sms", "production", "eu-west"]
)
#+end_src

It registers with the mesh. Consumers asking for =send_notification= with =+sms= and =eu-west= preference now route there. No config changes to any consumer.

*Replacing a provider*

Your old email service is deprecated. You deploy a new one:

#+begin_src python
@mesh.tool(
    capability="send_notification",
    version="2.0.0",
    tags=["email", "production"]
)
#+end_src

Consumers with =version: ">=2.0.0"= automatically route to the new one. Consumers with =version: "<2.0.0"= stay on the old one until you're ready to migrate.

*Removing a provider*

You shut down the old email service. The mesh:
1. Detects it's unhealthy (missed heartbeats)
2. Removes it from topology
3. Routes all =email= traffic to remaining providers

No consumer restarts. No config updates. It just happens.

*** Capability vs Endpoint: A Comparison

| Aspect | Endpoint (REST) | Capability (MCP Mesh) |
|--------|-----------------|----------------------|
| Identity | URL (location) | Name + tags + version |
| Discovery | Config/registry lookup | Automatic mesh resolution |
| Coupling | Tight (URL embedded) | Loose (only capability name) |
| Multiple providers | Load balancer config | Tag-based routing |
| Version migration | URL changes or headers | Version constraints |
| Adding provider | Config updates everywhere | Just deploy |
| Removing provider | Config updates everywhere | Just stop |

*** The Mental Shift

Stop thinking: "I need to call the user service at this URL"

Start thinking: "I need the =user_profile= capability with =+v2= tag"

This isn't just semantics. It changes:
- *How you deploy*: New services are instantly discoverable
- *How you scale*: Add instances, mesh routes automatically
- *How you evolve*: Version and tag your way through migrations
- *How you fail*: Mesh handles provider unavailability

*** What's Next

Capabilities are the abstraction. But how does the mesh actually resolve them?

The next article explains the registry â€” and why it's a facilitator, not an orchestrator. The distinction matters more than you'd think.

ðŸ‘‰ [Next: The registry as facilitator, not orchestrator]


* Article 3: The Registry as Facilitator
:PROPERTIES:
:EXPORT_TITLE: The Registry as Facilitator, Not Orchestrator
:EXPORT_TAGS: mcp, distributed-systems, architecture, registry
:END:

** Meta

- *Goal*: Explain why MCP Mesh registry is fundamentally different from orchestrators
- *Tone*: Architectural, with clear diagrams
- *Length*: ~1400 words
- *CTA*: "Next: Zero-downtime evolution"

** Article Draft

*** Introduction

Most agent frameworks have a central orchestrator:

#+begin_src
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Orchestrator â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼              â–¼              â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Agent  â”‚     â”‚ Agent  â”‚     â”‚ Agent  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#+end_src

The orchestrator creates agents, controls them, routes all communication through itself. It's the brain. Agents are limbs.

MCP Mesh takes a different view: agents are intelligent. Let them behave that way.

*** The Facilitator Model

#+begin_src
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                    Registry                          â”‚
     â”‚           (Facilitates, doesn't control)            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ heartbeat    â”‚ heartbeat    â”‚ heartbeat
                  â–¼              â–¼              â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ Agent  â”‚â—„â”€â”€â”€â–ºâ”‚ Agent  â”‚â—„â”€â”€â”€â–ºâ”‚ Agent  â”‚
             â”‚   A    â”‚     â”‚   B    â”‚     â”‚   C    â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      Direct communication
#+end_src

The registry:
- *Accepts* registrations (agents tell it what they provide)
- *Stores* capability metadata
- *Resolves* dependencies (tells agents where to find capabilities)
- *Monitors* health (tracks heartbeats)

The registry does *not*:
- Create or destroy agents
- Route messages between agents
- Control agent behavior
- Make decisions for agents

*** How Communication Works

*Step 1: Registration*

Agent A starts up and sends a heartbeat:

#+begin_src json
{
  "agent_id": "agent-a-uuid",
  "capabilities": [
    {"name": "process_data", "tags": ["batch", "production"]}
  ],
  "endpoint": "http://agent-a:9000",
  "dependencies": [
    {"capability": "store_result"}
  ]
}
#+end_src

*Step 2: Topology Response*

Registry responds with the current topology:

#+begin_src json
{
  "topology_version": 42,
  "dependencies": {
    "store_result": {
      "endpoint": "http://agent-b:9001",
      "agent_id": "agent-b-uuid"
    }
  }
}
#+end_src

Agent A now knows how to reach =store_result=.

*Step 3: Direct Communication*

When Agent A needs to store a result:

#+begin_src
Agent A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Agent B
         Direct MCP call (not through registry)
#+end_src

The registry is not in the communication path. Agents talk directly.

*** Why This Matters

*No single point of failure for communication*

If the registry goes down:
- Existing agents keep their last-known topology
- Agents can still communicate with each other
- New registrations fail, but existing mesh continues

Compare to an orchestrator model where the orchestrator dying means all communication stops.

*No bottleneck*

Registry handles lightweight heartbeats (~200 bytes every 5 seconds). Actual data flows directly between agents. The registry doesn't see your payloads, doesn't process your requests, doesn't become a scaling bottleneck.

*Agents are autonomous*

Agents decide when to call other agents. Agents handle their own retry logic (or use mesh defaults). Agents can cache topology and operate semi-independently.

This matches how intelligent agents *should* work. They're not puppets waiting for strings to be pulled.

*** The Heartbeat System

MCP Mesh uses dual heartbeats:

| Type | Frequency | Size | Purpose |
|------|-----------|------|---------|
| HEAD | ~5 seconds | ~200 bytes | "I'm alive" |
| POST | On topology change | ~2KB | Full registration |

Most heartbeats are lightweight HEADs. Full POSTs only happen when:
- Agent first starts
- Agent's capabilities change
- Registry topology version changes

This keeps registry load minimal while maintaining fast failure detection.

*** Failure Detection

#+begin_src
Timeline:
0s   - Agent A sends heartbeat
5s   - Agent A sends heartbeat
10s  - Agent A sends heartbeat
15s  - Agent A crashes
20s  - Missed heartbeat #1
25s  - Missed heartbeat #2
30s  - Missed heartbeat #3
35s  - Missed heartbeat #4 â†’ Agent A marked unhealthy
36s  - Topology updated, all agents notified
#+end_src

Sub-20 second failure detection. When Agent A goes unhealthy:
1. Registry updates topology version
2. Other agents' next heartbeat gets new topology
3. Dependencies on Agent A resolve to alternatives (or to =None= if no alternative)

*** Orchestrator vs Facilitator: Comparison

| Aspect | Orchestrator | Facilitator (MCP Mesh) |
|--------|--------------|------------------------|
| Creates agents | Yes | No (agents self-register) |
| Routes messages | Yes (all traffic) | No (agents talk directly) |
| Single point of failure | Yes | No (for communication) |
| Scaling bottleneck | Yes | No |
| Agent autonomy | Low | High |
| Complexity | High | Low |
| Recovery from failure | Hard (state in orchestrator) | Easy (state in agents) |

*** When You Need an Orchestrator

Facilitator model works when agents are truly autonomous â€” they know what to do, they just need to find collaborators.

You might need an orchestrator when:
- Agents are stateless workers with no decision-making
- You need strict workflow ordering (A â†’ B â†’ C always)
- Central state management is required
- Agents can't be trusted to self-organize

MCP Mesh supports both patterns. Use the registry as facilitator for autonomous agents. Add workflow tools on top if you need orchestration.

*** The Philosophical Difference

Traditional orchestrators treat agents like dumb workers:
"Do this. Now do that. Wait for me to tell you what's next."

MCP Mesh treats agents like intelligent collaborators:
"Here's who can help you. Figure out the rest."

If you're building AI agents â€” systems meant to be intelligent â€” which model makes more sense?

*** What's Next

The facilitator model enables something powerful: zero-downtime evolution. Add agents, remove agents, upgrade agents â€” all without restarts or reconfigs.

The next article shows how this works in practice.

ðŸ‘‰ [Next: Zero-downtime evolution â€” add agents without restarts]


* Article 4: Zero-Downtime Evolution
:PROPERTIES:
:EXPORT_TITLE: Zero-Downtime Evolution â€” Add Agents Without Restarts
:EXPORT_TAGS: devops, microservices, mcp, deployment
:END:

** Meta

- *Goal*: Show how MCP Mesh topology updates without restarts
- *Tone*: Practical, demo-style
- *Length*: ~1200 words
- *CTA*: "Week 2 complete â€” next week: building blocks"

** Article Draft

*** Introduction

Traditional microservices evolution:

1. Write new service
2. Update configs for every service that might use it
3. Plan deployment order (dependency graph)
4. Deploy new service
5. Restart dependent services (to pick up config)
6. Pray nothing breaks
7. Repeat for every environment

MCP Mesh evolution:

1. Write new agent
2. Deploy it
3. Done

Let me show you what this looks like.

*** Demo: Adding an Agent

We have a running mesh with two agents:

#+begin_src bash
$ meshctl list
NAME            STATUS    CAPABILITIES           UPTIME
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
job-matcher     healthy   match_jobs            2h 15m
resume-parser   healthy   parse_resume          2h 15m
#+end_src

The job-matcher currently has no scoring capability. Let's add one.

*Step 1: Write the agent*

#+begin_src python
# scoring_agent.py
from mcp_mesh import mesh

@mesh.agent(name="scoring-agent", port=9003)
@mesh.tool(
    capability="score_candidate",
    tags=["scoring", "production"]
)
async def score_candidate(
    resume_data: dict,
    job_requirements: list
) -> ScoreResult:
    # Scoring logic
    return ScoreResult(score=85, explanation="Strong match")
#+end_src

*Step 2: Start it*

#+begin_src bash
$ meshctl start scoring_agent.py
[INFO] Agent scoring-agent starting on port 9003
[INFO] Registered with mesh registry
[INFO] Capabilities: [score_candidate]
#+end_src

*Step 3: Check the mesh*

#+begin_src bash
$ meshctl list
NAME            STATUS    CAPABILITIES           UPTIME
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
job-matcher     healthy   match_jobs            2h 16m
resume-parser   healthy   parse_resume          2h 16m
scoring-agent   healthy   score_candidate       12s
#+end_src

That's it. The scoring agent is now discoverable.

*** What Happened Behind the Scenes

#+begin_src
Timeline:
0.0s  - scoring-agent starts
0.1s  - Sends registration heartbeat to registry
0.2s  - Registry adds to capability index
0.3s  - Registry increments topology version (42 â†’ 43)
5.0s  - job-matcher sends regular heartbeat
5.1s  - Registry responds with new topology (version 43)
5.2s  - job-matcher now knows about score_candidate
#+end_src

Within one heartbeat cycle (~5 seconds), every agent knows about the new capability.

*** Consuming the New Capability

The job-matcher can now use scoring. Update its code:

#+begin_src python
@mesh.tool(
    capability="match_jobs",
    dependencies=[
        {"capability": "score_candidate"}  # New dependency
    ]
)
async def match_jobs(
    candidate_id: str,
    score_candidate: McpMeshAgent = None  # Will be injected
) -> MatchResult:

    resume = await get_resume(candidate_id)

    if score_candidate:  # Gracefully handle if unavailable
        score = await score_candidate(
            resume_data=resume,
            job_requirements=job.requirements
        )

    return MatchResult(...)
#+end_src

Restart job-matcher (the only restart needed â€” because *its* code changed):

#+begin_src bash
$ meshctl start job_matcher.py  # Replaces running instance
#+end_src

The =score_candidate= dependency is automatically resolved to the scoring-agent we just deployed.

*** Demo: Upgrading an Agent

The scoring agent needs an update. We want v2 with new features.

*Step 1: Deploy v2 alongside v1*

#+begin_src python
# scoring_agent_v2.py
@mesh.tool(
    capability="score_candidate",
    version="2.0.0",
    tags=["scoring", "production", "v2"]
)
async def score_candidate(
    resume_data: dict,
    job_requirements: list,
    include_breakdown: bool = False  # New parameter
) -> ScoreResult:
    ...
#+end_src

#+begin_src bash
$ meshctl start scoring_agent_v2.py --port 9004
#+end_src

*Step 2: Both versions are now available*

#+begin_src bash
$ meshctl list
NAME              STATUS    CAPABILITIES           VERSION  UPTIME
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
job-matcher       healthy   match_jobs            1.0.0    2h 20m
resume-parser     healthy   parse_resume          1.0.0    2h 20m
scoring-agent     healthy   score_candidate       1.0.0    5m
scoring-agent-v2  healthy   score_candidate       2.0.0    10s
#+end_src

Agents requesting =score_candidate= without version constraints get load-balanced across both. Agents requesting =version: ">=2.0.0"= get only v2.

*Step 3: Migrate consumers*

Update job-matcher to prefer v2:

#+begin_src python
dependencies=[
    {"capability": "score_candidate", "version": ">=2.0.0"}
]
#+end_src

*Step 4: Retire v1*

Once all consumers are on v2:

#+begin_src bash
$ meshctl stop scoring-agent  # Original v1 instance
#+end_src

The mesh detects it's gone, updates topology, routing continues to v2 only.

*** Demo: Handling Failure

What happens when an agent crashes?

#+begin_src bash
# Simulate crash
$ kill -9 $(pgrep -f scoring_agent_v2)
#+end_src

#+begin_src
Timeline:
0s   - scoring-agent-v2 crashes
5s   - Missed heartbeat #1
10s  - Missed heartbeat #2
15s  - Missed heartbeat #3
20s  - Missed heartbeat #4 â†’ marked unhealthy
21s  - Topology updated
25s  - job-matcher heartbeat receives new topology
26s  - job-matcher's score_candidate dependency â†’ None
#+end_src

The mesh handles this gracefully:

#+begin_src python
async def match_jobs(
    score_candidate: McpMeshAgent = None
):
    if score_candidate:
        score = await score_candidate(...)
    else:
        # Graceful degradation - continue without scoring
        score = None
#+end_src

No exceptions. No cascading failures. The system degrades gracefully.

*** The Zero-Downtime Patterns

| Pattern | How It Works |
|---------|--------------|
| *Add capability* | Deploy new agent â†’ instant discovery |
| *Replace capability* | Deploy new version â†’ route via tags/version |
| *Upgrade capability* | Run both versions â†’ migrate consumers â†’ retire old |
| *Remove capability* | Stop agent â†’ mesh updates â†’ consumers adapt |
| *Recover from failure* | Agent restarts â†’ re-registers â†’ mesh restores |

None of these require:
- Config file changes
- Deployment ordering
- Consumer restarts (unless their code changes)
- Manual intervention

*** The Compound Effect

Each pattern is simple. The compound effect is powerful.

In the hiring platform case study (Part 0), we:
- Added 3 agents mid-sprint
- Swapped LLM providers twice
- Upgraded the resume parser with no downtime
- Had one agent crash in staging (recovered in 30 seconds)

None of this required coordinated deployments or config management. The mesh handled it.

*** What's Next

Week 2 covered the paradigm shift:
- Capabilities replace endpoints
- Registry facilitates, doesn't orchestrate
- Evolution happens without restarts

Week 3 goes deeper into the building blocks: dependency injection, heartbeats, tags, and LLMs as capabilities.

ðŸ‘‰ [Next week: Building Blocks â€” how MCP Mesh actually works]


* Revision Notes

** Article 2 (Capabilities)
- [ ] Add diagram showing capability resolution flow
- [ ] Include real meshctl output for capability listing
- [ ] Add example of capability schema introspection

** Article 3 (Registry as Facilitator)
- [ ] Add sequence diagram for registration flow
- [ ] Include actual heartbeat payload examples
- [ ] Clarify registry HA story (PostgreSQL backend)

** Article 4 (Zero-Downtime)
- [ ] Record actual terminal session for demos
- [ ] Add timing measurements from real deployment
- [ ] Include rollback scenario

** Publishing Checklist
- [ ] Ensure Part 0 and Article 1 are live before publishing these
- [ ] Cross-reference back to case study examples
- [ ] Add series navigation links
- [ ] Schedule: Article 2 Monday, Article 3 Wednesday, Article 4 Friday
