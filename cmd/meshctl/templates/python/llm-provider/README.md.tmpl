# {{ .Name }}

{{ if .Description }}{{ .Description }}{{ else }}A MCP Mesh LLM provider generated using `meshctl scaffold`.{{ end }}

## Overview

This is a zero-code LLM provider that exposes {{ .Model }} to other MCP Mesh agents.

## Getting Started

### Prerequisites

- Python 3.11+
- MCP Mesh SDK
- FastMCP
- LiteLLM
{{ if contains .Model "anthropic" }}- ANTHROPIC_API_KEY environment variable{{ else if contains .Model "openai" }}- OPENAI_API_KEY environment variable{{ else if contains .Model "gemini" }}- GOOGLE_API_KEY environment variable{{ end }}

### Installation

```bash
pip install -r requirements.txt
```

### Environment Variables

{{ if contains .Model "anthropic" }}
```bash
export ANTHROPIC_API_KEY="your-api-key"
```
{{ else if contains .Model "openai" }}
```bash
export OPENAI_API_KEY="your-api-key"
```
{{ else if contains .Model "gemini" }}
```bash
export GOOGLE_API_KEY="your-api-key"
```
{{ else }}
Set the appropriate API key for your LLM provider.
{{ end }}

### Running the Provider

```bash
meshctl start main.py
```

Or with debug logging:

```bash
meshctl start main.py --debug
```

The provider will start on port {{ .Port }} by default.

## Configuration

| Parameter | Value | Description |
|-----------|-------|-------------|
| Model | {{ .Model }} | LiteLLM model identifier |
| Port | {{ .Port }} | HTTP server port |
| Tags | {{ join .Tags ", " }} | Discovery tags |

## How It Works

This provider uses the `@mesh.llm_provider` decorator to automatically:

1. Create a `process_chat` function that handles LLM requests
2. Wrap LiteLLM with error handling and retries
3. Register with the mesh network for discovery
4. Handle tool calling and streaming responses

## Using This Provider

Other agents can use this provider with the `@mesh.llm` decorator:

```python
@mesh.llm(
    provider={"capability": "llm", "tags": {{ toJSON .Tags }}},
    max_iterations=1,
    system_prompt="You are a helpful assistant.",
    context_param="ctx",
)
@mesh.tool(...)
def my_llm_tool(ctx: MyContext, llm: mesh.MeshLlmAgent = None):
    return llm("Process this request")
```

## Project Structure

```
{{ .Name }}/
├── __init__.py       # Package init
├── __main__.py       # Module entry point
├── main.py           # Provider implementation
├── README.md         # This file
└── requirements.txt  # Python dependencies
```

## Health Check

The provider includes a health check that validates:
{{ if contains .Model "anthropic" }}
- ANTHROPIC_API_KEY is set
- Anthropic API is reachable
{{ else if contains .Model "openai" }}
- OPENAI_API_KEY is set
- OpenAI API is reachable
{{ else if contains .Model "gemini" }}
- GOOGLE_API_KEY is set
- Google Gemini API is reachable
{{ else }}
- Provider is configured correctly
{{ end }}

Health status is cached for 30 seconds.

## License

MIT
