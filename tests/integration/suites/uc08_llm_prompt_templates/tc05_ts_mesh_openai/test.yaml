# Test Case: tc05_ts_mesh_openai
# Tests TypeScript LLM agent with context_param using OpenAI provider mesh delegation
# Verifies context_param values work with TypeScript SDK and OpenAI LLM

name: "Context Self-Dependency - TypeScript with OpenAI"
description: "Test TypeScript LLM agent with context_param using OpenAI provider delegation"
tags:
  - llm
  - openai
  - typescript
  - mesh-delegation
  - prompt-template
  - context-param
timeout: 300

pre_run:
  - routine: global.setup_for_typescript_agent
    params:
      meshctl_version: "${config.packages.cli_version}"

test:
  # Copy all UC artifacts to workspace
  - name: "Copy artifacts to workspace"
    handler: shell
    command: |
      cp -r /uc-artifacts/* /workspace/
    capture: copy_output

  # Create env file with secrets
  - name: "Create env file with secrets"
    handler: secrets
    target: /workspace/.env

  # Verify workspace files
  - name: "Verify workspace files"
    handler: shell
    command: "ls -la /workspace/"
    capture: ls_output

  # Install OpenAI provider dependencies
  - name: "Install openai-provider-ts dependencies"
    handler: npm-install
    path: /workspace/openai-provider-ts

  # Install TypeScript agent dependencies
  - name: "Install context-self-dep-ts-mesh dependencies"
    handler: npm-install
    path: /workspace/context-self-dep-ts-mesh

  # Start OpenAI provider first (mesh delegation target)
  - name: "Start openai-provider-ts"
    handler: shell
    command: "meshctl start openai-provider-ts/src/index.ts --env-file .env -d"
    workdir: /workspace
    capture: provider_start_output

  # Wait for provider to register
  - name: "Wait for provider to register"
    handler: wait
    seconds: 15

  # Verify provider is running
  - name: "Verify provider is running"
    handler: shell
    command: "meshctl list"
    workdir: /workspace
    capture: provider_list

  # Start the TypeScript mesh agent (depends on provider)
  - name: "Start context-self-dep-ts-mesh agent"
    handler: shell
    command: "meshctl start context-self-dep-ts-mesh/src/index.ts -d"
    workdir: /workspace
    capture: agent_start_output

  # Wait for agent to register and resolve dependencies
  - name: "Wait for agent to register"
    handler: wait
    seconds: 15

  # Verify both agents are running
  - name: "Verify agents are running"
    handler: shell
    command: "meshctl list"
    workdir: /workspace
    capture: agent_list

  # Verify provider resolution in mesh
  - name: "Verify provider resolution"
    handler: shell
    command: "meshctl status"
    workdir: /workspace
    capture: status_output

  # List available tools
  - name: "List available tools"
    handler: shell
    command: "meshctl list -t"
    workdir: /workspace
    capture: tools_output

  # Call the TypeScript agent with context_param
  # This tests: TypeScript agent -> mesh.llm() -> OpenAI provider with context_param
  - name: "Call context_self_dep_ts_mesh with context"
    handler: shell
    command: |
      meshctl call context_self_dep_ts_mesh '{"ctx": {"inputText": "What is 7+9?", "userName": "TestUserCharlie"}}'
    workdir: /workspace
    capture: test_result

assertions:
  # Provider registered successfully
  - expr: ${captured.provider_list} contains 'openai-provider-ts'
    message: "openai-provider-ts should appear in meshctl list"

  # TypeScript agent registered successfully
  - expr: ${captured.agent_list} contains 'context-self-dep-ts-mesh'
    message: "context-self-dep-ts-mesh agent should appear in meshctl list"

  # Provider resolved in mesh (TypeScript provider exposes 'llm' capability)
  - expr: ${captured.status_output} contains 'process_chat'
    message: "OpenAI provider tool (process_chat) should be resolved in mesh status"

  # Tools are registered
  - expr: ${captured.tools_output} contains 'context_self_dep_ts_mesh'
    message: "context_self_dep_ts_mesh tool should be listed"

  # TypeScript OpenAI provider exposes 'process_chat' tool (not openai_provider)
  - expr: ${captured.tools_output} contains 'process_chat'
    message: "process_chat tool should be listed from OpenAI provider"

  # Key assertion: userName from context must appear in response
  # This proves context_param works through: TypeScript -> mesh -> OpenAI
  - expr: ${jq:captured.test_result:.content[0].text} contains 'TestUserCharlie'
    message: "Response should contain userName from context (proves context_param works with OpenAI)"

  # Verify LLM actually responded with answer
  - expr: ${jq:captured.test_result:.content[0].text} contains '16'
    message: "LLM response should contain the answer 16"

post_run:
  - handler: shell
    command: "meshctl stop 2>/dev/null || true"
    workdir: /workspace
    ignore_errors: true
  - routine: global.cleanup_workspace
